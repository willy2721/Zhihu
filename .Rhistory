library(readr)
library(dplyr)
library(jiebaR)
library(tidytext)
library(fpc)
library(cluster)
library(rJava)
library(tm)
library(SnowballC)
library(slam)
library(XML)
library(RCurl)
library(Matrix)
library(tmcn)
library(Rwordseg)
source('~/dsR/final/script/zhihu_utility.R')
source('~/dsR/final/script/zhihu_count.R')
source('~/dsR/final/script/zhihu_senti.R')
source('~/dsR/final/script/zhihu_tfidf_score.R')
#source('~/dsR/final/script/zhihu_tf_idf.R')
source('~/dsR/final/script/zhihu_cluster.R')
# Global variables
data <- tbl_df(read_csv("~/dsR/final/data/current_event.csv")) %>% na.omit()
#data <- data[1:50, ]
data <- data[1:50, ]
# Clean entire datum
#data <- full_clean(data)
#data <- subset(data, data$ans_upvote_num != 0)
#data <- text_filter(data)
data <- data[1:50, ]
#Reorder the data
data <- data[order(data$question_title),]
# Get stop words
data$question_combined <- paste(data$question_title, data$question_detail)
document <- c(unique(data$question_combined),unique(data$ans))
stop_word <- get_stop_word(document)
stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
# Add response time
data$response_time <- time_transform(data)
# Add segmented questions and answers
data$ans_seg <- sapply(data$ans, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
data$ans_seg_vec <- sapply(data$ans, function(x) filter_segment(seg_worker[x], stop_word))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
# Add number of characters, words, and percentage of stop words
data$n_char <- char_count(data)
data$n_word <- word_count(data)
data$n_stop <- stop_word_count(data,stop_word)
data$per_stop <- stop_word_percentage(data)
# Add sentiment score
data$senti_score <- senti(data)
# Add tf_idf
ans_tfidf <- by(data, data$question_title, tf_idf_score)
data$ans_tfidf <- unlist(ans_tfidf)
# Add cluster
data <- get_cluster_feature(data)
# Add answer quality
ans_quality <- by(data, data$question_title, get_ans_quality)
data$quality <- unlist(ans_quality)
head(data,20)
#Remove the vector column and 0
data$ans_seg_vec <- NULL
data <- subset(data, data$ans_upvote_num != 0)
testset <- take_sample(data)
trainset <- anti_join(data,testset)
(tuned <- tune.svm(quality~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2), probability = TRUE))
source('~/dsR/final/script/zhihu_utility.R')
source('~/dsR/final/script/zhihu_count.R')
source('~/dsR/final/script/zhihu_senti.R')
source('~/dsR/final/script/zhihu_tfidf_score.R')
#source('~/dsR/final/script/zhihu_tf_idf.R')
source('~/dsR/final/script/zhihu_cluster.R')
# Global variables
data <- tbl_df(read_csv("~/dsR/final/data/current_event.csv")) %>% na.omit()
# Clean entire datum
data <- full_clean(data)
#data <- subset(data, data$ans_upvote_num != 0)
#data <- text_filter(data)
data <- data[1:50, ]
#Reorder the data
data <- data[order(data$question_title),]
# Get stop words
data$question_combined <- paste(data$question_title, data$question_detail)
document <- c(unique(data$question_combined),unique(data$ans))
stop_word <- get_stop_word(document)
stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
# Add response time
data$response_time <- time_transform(data)
# Add segmented questions and answers
data$ans_seg <- sapply(data$ans, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
data$ans_seg_vec <- sapply(data$ans, function(x) filter_segment(seg_worker[x], stop_word))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
# Add number of characters, words, and percentage of stop words
data$n_char <- char_count(data)
data$n_word <- word_count(data)
data$n_stop <- stop_word_count(data,stop_word)
data$per_stop <- stop_word_percentage(data)
# Add sentiment score
data$senti_score <- senti(data)
# Add tf_idf
ans_tfidf <- by(data, data$question_title, tf_idf_score)
data$ans_tfidf <- unlist(ans_tfidf)
# Add cluster
data <- get_cluster_feature(data)
# Add answer quality
ans_quality <- by(data, data$question_title, get_ans_quality)
data$quality <- unlist(ans_quality)
head(data,20)
#Remove the vector column and 0
data$ans_seg_vec <- NULL
data <- subset(data, data$ans_upvote_num != 0)
install.packages("e1071")
library(e1071)
install.packages("MLmetrics")
library(MLmetrics)
#Remove the vector column and 0
data$ans_seg_vec <- NULL
data <- subset(data, data$ans_upvote_num != 0)
testset <- take_sample(data)
trainset <- anti_join(data,testset)
(tuned <- tune.svm(quality~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2), probability = TRUE))
